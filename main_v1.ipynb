{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "import project_tests as tests\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "from matplotlib.pyplot import imshow, tight_layout\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    \"\"\"\n",
    "    Load Pretrained VGG Model into TensorFlow.\n",
    "    :param sess: TensorFlow Session\n",
    "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
    "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    #   Use tf.saved_model.loader.load to load the model and weights\n",
    "    vgg_tag = 'vgg16'\n",
    "    vgg_input_tensor_name = 'image_input:0'\n",
    "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
    "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
    "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
    "    \n",
    "    # load any normal CNN architecture through its path\n",
    "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    # get the layers of vgg to use in Fully Convolutional Network FCN-8s\n",
    "    input_ = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "    layer3 = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "    layer4 = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "    layer7 = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "    \n",
    "    return input_, keep_prob, layer3, layer4, layer7\n",
    "\n",
    "tests.test_load_vgg(load_vgg, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
    "    \"\"\"\n",
    "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
    "    :param vgg_layer7_out: TF Tensor for VGG Layer 3 output\n",
    "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
    "    :param vgg_layer3_out: TF Tensor for VGG Layer 7 output\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: The Tensor for the last layer of output\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    \n",
    "    # go through conv 1x1 for each layer to have the same shape with upsampled layers\n",
    "    # so we can add them together\n",
    "    # kernel_initializer=tf.truncated_normal_initializer(stddev=0.01)\n",
    "    layer_3 = tf.layers.conv2d(vgg_layer3_out, num_classes, kernel_size=1, padding='same',\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    layer_4 = tf.layers.conv2d(vgg_layer4_out, num_classes, kernel_size=1, padding='same',\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    layer_7 = tf.layers.conv2d(vgg_layer7_out, num_classes, kernel_size=1, padding='same',\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    \n",
    "    # scaling layers before feeding into conv 1x1\n",
    "    layer_3 = tf.multiply(layer_3, 0.0001)\n",
    "    layer_4 = tf.multiply(layer_4, 0.01)\n",
    "\n",
    "    decoding_layer_1 = tf.layers.conv2d_transpose(layer_7, num_classes, \n",
    "        kernel_size=4, strides=2, padding='same', \n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    decoding_layer_1 = tf.add(decoding_layer_1, layer_4)\n",
    "    \n",
    "    decoding_layer_2 = tf.layers.conv2d_transpose(decoding_layer_1, num_classes, \n",
    "        kernel_size=4, strides=2, padding='same', \n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    decoding_layer_2 = tf.add(decoding_layer_2, layer_3)\n",
    "    \n",
    "    decoding_layer_3 = tf.layers.conv2d_transpose(decoding_layer_2, num_classes, \n",
    "        kernel_size=16, strides=8, padding='same',\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    \n",
    "    return decoding_layer_3\n",
    "\n",
    "tests.test_layers(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
    "    \"\"\"\n",
    "    Build the TensorFLow loss and optimizer operations.\n",
    "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "    :param correct_label: TF Placeholder for the correct label image\n",
    "    :param learning_rate: TF Placeholder for the learning rate\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    # Add regularization loss and cross entropy loss together\n",
    "    regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    regularization_loss = sum(regularization_losses)\n",
    "    \n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes), name='logits')\n",
    "    labels = tf.reshape(correct_label, (-1, num_classes))\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    cross_entropy_loss = tf.add(cross_entropy_loss, regularization_loss)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "    \n",
    "    return logits, train_op, cross_entropy_loss\n",
    "\n",
    "tests.test_optimize(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./data/vgg/variables/variables'\n"
     ]
    }
   ],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate):\n",
    "    \"\"\"\n",
    "    Train neural network and print out the loss during training.\n",
    "    :param sess: TF Session\n",
    "    :param epochs: Number of epochs\n",
    "    :param batch_size: Batch size\n",
    "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "    :param train_op: TF Operation to train the neural network\n",
    "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
    "    :param input_image: TF Placeholder for input images\n",
    "    :param correct_label: TF Placeholder for label images\n",
    "    :param keep_prob: TF Placeholder for dropout keep probability\n",
    "    :param learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    training_losses = []\n",
    "    for i in range(epochs):\n",
    "        start_time = time.clock()\n",
    "        training_loss = 0\n",
    "        for image, label in get_batches_fn(batch_size):\n",
    "            _, loss = sess.run([train_op, cross_entropy_loss],\n",
    "                feed_dict={input_image: image, correct_label: label, keep_prob: 0.5, learning_rate: 0.0001})\n",
    "            training_loss += loss\n",
    "            \n",
    "        training_loss = training_loss / batch_size\n",
    "        training_losses.append(training_loss)\n",
    "        end_time = time.clock()\n",
    "        print('Epoch {} took {} seconds. Training error was {}.'.format(i+1, end_time-start_time, training_loss))\n",
    "    return training_losses\n",
    "\n",
    "tests.test_train_nn(train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/tfolkman/CarND-Semantic-Segmentation/blob/master/main_notebook.ipynb\n",
    "def process_image(image, sess, keep_prob, logits, input_image, image_shape):\n",
    "    \n",
    "    image = scipy.misc.imresize(image, image_shape)\n",
    "    \n",
    "    im_softmax = sess.run(\n",
    "            [tf.nn.softmax(logits)],\n",
    "            {keep_prob: 1.0, input_image: [image]})\n",
    "        \n",
    "\n",
    "    label_index = np.argmax(im_softmax, axis=2)\n",
    "\n",
    "    value_fill_1 = label_index.copy()\n",
    "    value_fill_1.fill(1)\n",
    "    value_fill_2 = label_index.copy()\n",
    "    value_fill_2.fill(2)\n",
    "    value_fill_3 = label_index.copy()\n",
    "    value_fill_3.fill(3)\n",
    "\n",
    "    segmentation1 = np.equal(label_index, value_fill_1)\n",
    "    segmentation2 = np.equal(label_index, value_fill_2)\n",
    "    segmentation3 = np.equal(label_index, value_fill_3)\n",
    "    \n",
    "    segmentation1 = segmentation1.reshape(image_shape[0], image_shape[1], 1)\n",
    "    segmentation2 = segmentation2.reshape(image_shape[0], image_shape[1], 1)\n",
    "    segmentation3 = segmentation3.reshape(image_shape[0], image_shape[1], 1)\n",
    "    mask1 = np.dot(segmentation1, np.array([[0, 255, 0, 127]]))\n",
    "    mask1 = scipy.misc.toimage(mask1, mode=\"RGBA\")\n",
    "    mask2 = np.dot(segmentation2, np.array([[0, 0, 255, 127]]))\n",
    "    mask2 = scipy.misc.toimage(mask2, mode=\"RGBA\")\n",
    "    mask3 = np.dot(segmentation3, np.array([[255, 242, 0, 127]]))\n",
    "    mask3 = scipy.misc.toimage(mask3, mode=\"RGBA\")\n",
    "    street_im = scipy.misc.toimage(image)\n",
    "    street_im.paste(mask1, box=None, mask=mask1)\n",
    "    street_im.paste(mask2, box=None, mask=mask2)\n",
    "    street_im.paste(mask3, box=None, mask=mask3)\n",
    "    \n",
    "    return np.array(street_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # operation flags\n",
    "    retrain = True\n",
    "    predict_image = False\n",
    "    predict_video = False\n",
    "    \n",
    "    tf.reset_default_graph()   # need to reset before run a new model\n",
    "    num_classes = 2\n",
    "    image_shape = (160, 576)\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    tests.test_for_kitti_dataset(data_dir)\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
    "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
    "    #  https://www.cityscapes-dataset.com/\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "\n",
    "        if retrain:\n",
    "            # Path to vgg model\n",
    "            vgg_path = os.path.join(data_dir, 'vgg')\n",
    "            # Create function to get batches\n",
    "            get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
    "\n",
    "            # OPTIONAL: Augment Images for better results\n",
    "            #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
    "\n",
    "            # TODO: Build NN using load_vgg, layers, and optimize function\n",
    "            epochs = 50\n",
    "            batch_size = 6\n",
    "\n",
    "            # TF placeholders\n",
    "            correct_label = tf.placeholder(tf.int32, [None, None, None, num_classes], name='correct_label')\n",
    "            learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "            input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)\n",
    "            nn_last_layer = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
    "            logits, train_op, cross_entropy_loss = optimize(nn_last_layer, correct_label, learning_rate, num_classes)\n",
    "\n",
    "            # TODO: Train NN using the train_nn function\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "            saver = tf.train.Saver()\n",
    "            training_losses = train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
    "                correct_label, keep_prob, learning_rate)\n",
    "            saver.save(sess, 'segmentation_model')\n",
    "            \n",
    "            # TODO: Save inference data using helper.save_inference_samples\n",
    "            helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n",
    "        \n",
    "        if predict_image:\n",
    "            # Load model\n",
    "            saver = tf.train.import_meta_graph('segmentation_model.meta')\n",
    "            saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "            \n",
    "            # Extract tensors\n",
    "            graph = tf.get_default_graph()\n",
    "            input_image = graph.get_tensor_by_name('image_input:0')\n",
    "            keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "            logits = graph.get_tensor_by_name('logits:0')\n",
    "            \n",
    "            img = scipy.misc.imread(\"./data/data_road/training/image_2/um_000007.png\")\n",
    "            output_img = process_image(img, sess, keep_prob, logits, input_image, image_shape)\n",
    "            imshow(output_img)\n",
    "            tight_layout()\n",
    "            \n",
    "        # OPTIONAL: Apply the trained model to a video\n",
    "        \n",
    "        if predict_video:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "            output_video = 'driving_output.mp4'\n",
    "            input_video = 'driving.mp4'\n",
    "            \n",
    "            partial_process_image = partial(process_image,  sess=sess, logits=logits, keep_prob=keep_prob,\n",
    "                                            input_image=vgg_input, image_shape=image_shape)\n",
    "\n",
    "            clip1 = VideoFileClip(input_video)            \n",
    "            video_clip = clip1.fl_image(partial_process_image)\n",
    "            %time video_clip.write_videofile(output_video, audio=False)\n",
    "        \n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    training_losses = run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch 1 took 33.294155000000046 seconds. Training error was 4.355113133788109.\n",
    "Epoch 2 took 32.79390999999987 seconds. Training error was 2.294324889779091.\n",
    "Epoch 3 took 32.78882199999998 seconds. Training error was 1.6767770536243916.\n",
    "Epoch 4 took 33.59218800000008 seconds. Training error was 1.5267454460263252.\n",
    "Epoch 5 took 33.40632900000037 seconds. Training error was 1.4614848233759403.\n",
    "Epoch 6 took 33.845368000000235 seconds. Training error was 1.3017204776406288.\n",
    "Epoch 7 took 32.85208799999964 seconds. Training error was 1.219868490472436.\n",
    "Epoch 8 took 33.022003999999924 seconds. Training error was 1.1810611486434937.\n",
    "Epoch 9 took 32.95940900000005 seconds. Training error was 1.1146413162350655.\n",
    "Epoch 10 took 32.82030600000007 seconds. Training error was 1.0735535342246294.\n",
    "\n",
    "Epoch 11 took 32.837132999999994 seconds. Training error was 1.0871983636170626.\n",
    "Epoch 12 took 32.96257400000013 seconds. Training error was 1.0119769256561995.\n",
    "Epoch 13 took 33.318541000000096 seconds. Training error was 0.9560791347175837.\n",
    "Epoch 14 took 33.16982400000006 seconds. Training error was 0.9342574644833803.\n",
    "Epoch 15 took 33.057060999999976 seconds. Training error was 0.9102634228765965.\n",
    "Epoch 16 took 32.75053500000013 seconds. Training error was 0.8956393226981163.\n",
    "Epoch 17 took 33.13380099999995 seconds. Training error was 0.8831686731427908.\n",
    "Epoch 18 took 33.01841599999989 seconds. Training error was 0.8397952504456043.\n",
    "Epoch 19 took 33.354913000000124 seconds. Training error was 0.8161631133407354.\n",
    "Epoch 20 took 33.174782999999934 seconds. Training error was 0.7997701484709978.\n",
    "\n",
    "Epoch 21 took 33.21742499999982 seconds. Training error was 0.7759448476135731.\n",
    "Epoch 22 took 33.45178900000019 seconds. Training error was 0.7640944216400385.\n",
    "Epoch 23 took 32.90679999999975 seconds. Training error was 0.7495317533612251.\n",
    "Epoch 24 took 33.175846999999976 seconds. Training error was 0.7308971844613552.\n",
    "Epoch 25 took 33.099031000000195 seconds. Training error was 0.715748380869627.\n",
    "Epoch 26 took 33.07040899999993 seconds. Training error was 0.6972469165921211.\n",
    "Epoch 27 took 32.765405999999984 seconds. Training error was 0.6919837463647127.\n",
    "Epoch 28 took 32.96255999999994 seconds. Training error was 0.7177233286201954.\n",
    "Epoch 29 took 33.16145199999983 seconds. Training error was 0.6936861649155617.\n",
    "Epoch 30 took 33.04784900000004 seconds. Training error was 0.6718434989452362.\n",
    "\n",
    "Training Finished.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
